model:
  name: "answerdotai/ModernBERT-base"
  output_dir: "checkpoints"
  max_length: 256
  dropout: 0.1
  # --- Pooling Strategy --- #
  # Options: "cls", "mean", "cls_mean_concat", "weighted_layer", "cls_weighted_concat"
  # "cls" uses just the [CLS] token for classification
  # "mean" uses mean pooling over final hidden states for classification
  # "cls_mean_concat" uses both [CLS] and mean pooling over final hidden states for classification
  # "weighted_layer" uses a weighted combination of the final hidden states from the top N layers for classification
  # "cls_weighted_concat" uses a weighted combination of the final hidden states from the top N layers and the [CLS] token for classification
  
  pooling_strategy: "cls_mean_concat" # Current default, change as needed
  num_weighted_layers: 4 # Number of top BERT layers to use for 'weighted_layer' strategies (e.g., 1 to 12 for BERT-base)

data:
  # No specific data paths needed as we use HF datasets at the moment

training:
  epochs: 2
  batch_size: 16
  lr: 1e-5 # 2.0e-5
  weight_decay_rate: 0.02 # 0.01

inference:
  # Default path, can be overridden
  model_path: "checkpoints/best_model.pt" 
  # Using the same max_length as training for consistency
  max_length: 256


# "answerdotai/ModernBERT-base"
# "answerdotai/ModernBERT-large"