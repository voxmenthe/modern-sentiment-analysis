model:
  name: "microsoft/deberta-v3-small"
  model_type: "deberta" # Specify deberta
  # loss_function: ... (This will be ignored by DeBERTa unless you implement a custom wrapper)
  output_dir: "checkpoints_deberta"
  max_length: 256 # DeBERTa-v3-small example used 256
  dropout: 0.1 # This will be applied to deberta_config.hidden_dropout_prob
  # pooling_strategy: ... (This will be ignored by DeBERTa)
  # num_weighted_layers: ... (This will be ignored by DeBERTa)

data:
  # No specific data paths needed as we use HF datasets at the moment

training:
  epochs: 3 # Example from DeBERTa page
  batch_size: 8 # Example from DeBERTa page
  lr: 4.5e-5 # Example from DeBERTa page
  weight_decay_rate: 0.02 # 0.01
  resume_from_checkpoint: "" # "checkpoints/mean_epoch2_0.9361acc_0.9355f1.pt" # Path to checkpoint file, or empty to not resume

inference:
  # Default path, can be overridden
  model_path: "checkpoints/mean_epoch5_0.9575acc_0.9575f1.pt" 
  # Using the same max_length as training for consistency
  max_length: 880 # 256


# "answerdotai/ModernBERT-base"
# "answerdotai/ModernBERT-large"