model:
  name:  "answerdotai/ModernBERT-base" # "microsoft/deberta-v3-small" # The two supported base models are either "answerdotai/ModernBERT-base" or "microsoft/deberta-v3-small"
  model_type: "modernbert" # Specify "modernbert" or "deberta"
  loss_function:
    name: "SentimentWeightedLoss" # Options: "SentimentWeightedLoss", "SentimentFocalLoss"
    params:
      # Extra parameters for SentimentFocalLoss:
      gamma_focal: 1.0               # Default: 1.0. Modulates focus on hard/easy examples.
      label_smoothing_epsilon: 0.1   # Default: 0.05. Amount of label smoothing.

  output_dir: "checkpoints"
  max_length: 888 # Reduced from 880 for better efficiency while maintaining accuracy
  dropout: 0.07 # This will be applied to deberta_config.hidden_dropout_prob

# --- Pooling Strategy Options --- #
# Options: "cls", "mean", "cls_mean_concat", "weighted_layer", "cls_weighted_concat"
# "cls" uses just the [CLS] token for classification
# "mean" uses mean pooling over final hidden states for classification
# "cls_mean_concat" uses both [CLS] and mean pooling over final hidden states for classification
# "weighted_layer" uses a weighted combination of the final hidden states from the top N layers for classification
# "cls_weighted_concat" uses a weighted combination of the final hidden states from the top N layers and the [CLS] token for classification
pooling_strategy: "mean" # "mean" is typically faster than other strategies

# --- Weighted Layer Options - only used for the two weighted_layer strategies --- #
num_weighted_layers: 4 # Number of top BERT layers to use

data:
  # No specific data paths needed as we use HF datasets at the moment

training:
  epochs: 16
  batch_size: 32 # Increased from 16 for better throughput
  lr: 4e-6 # Learning rate
  weight_decay_rate: 0.014 # Weight decay for regularization
  resume_from_checkpoint: "" # Path to checkpoint file, or empty to not resume
  log_step_metrics: true  # Set to true to enable step-level metrics
  optimizer: "ForeachMuon" # Using optimized Muon optimizer for better performance
  log_metrics_every_n_steps: 500 # Log metrics every N global training steps
  gradient_accumulation_steps: 3 # Number of steps to accumulate gradients before updating parameters

inference:
  # Default path, can be overridden
  model_path: "checkpoints/mean_epoch5_0.9575acc_0.9575f1.pt" # available in the HF space files - can be downloaded from there see readme
  # Using the same max_length as training for consistency
  max_length: 512 # Reduced to match training max_length



# "answerdotai/ModernBERT-base"
# "answerdotai/ModernBERT-large"